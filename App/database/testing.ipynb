{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "import torch\n",
    "from transformers import AlbertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.albert = AlbertModel.from_pretrained('albert-base-v2')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.albert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.albert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n",
    "\n",
    "def import_data(model_file):\n",
    "  n_classes = 2\n",
    "  model = SentimentClassifier(n_classes)\n",
    "  model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n",
    "  return model\n",
    "\n",
    "def predict_class(model,review):\n",
    "  model.eval()  # Put the model in evaluation mode\n",
    "\n",
    "  # Load the tokenizer\n",
    "  tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "  text = review\n",
    "  # Tokenize the example text and create attention masks\n",
    "  inputs = tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=128,\n",
    "      padding='max_length',\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',  # Return PyTorch tensors\n",
    "      truncation=True\n",
    "  )\n",
    "\n",
    "  # Get the input IDs and attention mask\n",
    "  input_ids = inputs['input_ids']\n",
    "  attention_mask = inputs['attention_mask']\n",
    "\n",
    "  # Predict\n",
    "  with torch.no_grad():  # No need to compute gradients for predictions\n",
    "      outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "  # The first element in the outputs is the logits\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Process the outputs\n",
    "  probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "  # Get the highest probability class\n",
    "  predicted_class = torch.argmax(probabilities, dim=-1).numpy()\n",
    "\n",
    "  # first element of pytorch tensor in float\n",
    "  probabilities = probabilities.numpy().tolist()\n",
    "  return (round(max(probabilities)*100, 1), \"Positive\" if predicted_class == 1 else \"Negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = import_data(\"albert_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"bad game\"\n",
    "result = predict_class(model,review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63.7, 'Negative')\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
