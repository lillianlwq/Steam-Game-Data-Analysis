{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/28 15:44:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.config(\"spark.sql.debug.maxToStringFields\", 100)\n",
    "    .appName(\"reviews\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# data_filepath = \"../data/cleaned_steam_reviews/game_id={578080,271590,359550,105600,4000,252490,252950,218620,945360,292030}\"\n",
    "data_filepath = \"../data/cleaned_steam_reviews/game_id={294100,304390,812140,306130,391220,221380,262060,1289310,646570,552520}\"\n",
    "data_filepath = \"../data/cleaned_steam_reviews/game_id={294100,304390,812140}\"\n",
    "# data_filepath = \"../data/cleaned_steam_reviews\"\n",
    "steam_games_filepath = \"../data/cleaned_steam_games\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "steam_reviews = spark.read.parquet(data_filepath)\n",
    "steam_games = spark.read.parquet(steam_games_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "playtime = steam_reviews.select('app_id', 'author_steamid', 'author_playtime_forever') \\\n",
    "                            .where((col(\"author_steamid\") != F.lit(\"null\")) & (F.length(col(\"author_steamid\")) == 17) & (col(\"author_steamid\").rlike(\"^[0-9]+$\"))) \\\n",
    "                            .where(steam_reviews[\"author_playtime_forever\"].isNotNull() \\\n",
    "                                   & steam_reviews[\"app_id\"].isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "# Fill in the entries one by one\n",
    "\n",
    "d[\"author_playtime_forever\"] = playtime.approxQuantile(\"author_playtime_forever\",[0.01,0.99],0.25)\n",
    "\n",
    "# # looping through the columns, doing log(x+1) transformations\n",
    "# for col in df.columns:\n",
    "playtime_quantile = playtime.withColumn(\"author_playtime_forever\", \\\n",
    "                F.log(F.when(playtime[\"author_playtime_forever\"] < d[\"author_playtime_forever\"][0],d[\"author_playtime_forever\"][0])\\\n",
    "                .when(playtime[\"author_playtime_forever\"] > d[\"author_playtime_forever\"][1], d[\"author_playtime_forever\"][1])\\\n",
    "                .otherwise(playtime[\"author_playtime_forever\"] ) +1).alias(\"author_playtime_forever\"))\n",
    "\n",
    "mean = playtime_quantile.select(F.mean(playtime_quantile.author_playtime_forever)).collect()\n",
    "\n",
    "playtime_capped = playtime_quantile.withColumn(\"author_playtime_forever\", F.when(playtime_quantile.author_playtime_forever > mean[0][0]*2, mean[0][0]*2).otherwise(playtime_quantile.author_playtime_forever))\n",
    "playtime_scaled = playtime_capped.withColumn(\"author_playtime_forever\", playtime_capped.author_playtime_forever / (mean[0][0]*2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-----------------------+\n",
      "|app_id|   author_steamid|author_playtime_forever|\n",
      "+------+-----------------+-----------------------+\n",
      "|294100|76561198892712976|    0.32939984568544967|\n",
      "|294100|76561198053458049|     0.5159773746231011|\n",
      "|294100|76561198113534698|    0.38116279094434025|\n",
      "|294100|76561198085554674|     0.5713611140256404|\n",
      "|294100|76561198096815495|    0.40600692664341137|\n",
      "|294100|76561198067122940|     0.6012115490074816|\n",
      "|294100|76561198394862196|     0.4244380936196737|\n",
      "|294100|76561198013936546|     0.5485325509397957|\n",
      "|294100|76561198027357642|    0.29842714632737116|\n",
      "|294100|76561198102039006|     0.5494378742649005|\n",
      "|294100|76561198029160051|     0.5061984091095405|\n",
      "|294100|76561198066472711|     0.4431199678578947|\n",
      "|294100|76561199117988906|     0.4339861542610619|\n",
      "|294100|76561198098384447|     0.5516572504132496|\n",
      "|294100|76561198019368906|     0.5821367917397706|\n",
      "|294100|76561198317032242|     0.5126770016774996|\n",
      "|294100|76561198079640725|     0.6499975619880852|\n",
      "|294100|76561198010498846|     0.5980766227321662|\n",
      "|294100|76561198130568315|     0.5542016162605026|\n",
      "|294100|76561198045407346|     0.6383717315791033|\n",
      "+------+-----------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "playtime_scaled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "playtime_data = (playtime_scaled\n",
    "    .select(\n",
    "        'app_id',\n",
    "        'author_steamid',\n",
    "        'author_playtime_forever',\n",
    "    )\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = playtime_data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/28 15:47:13 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:13 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:14 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:15 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:16 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:17 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:18 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:18 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:19 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:20 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:20 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:21 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:22 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:22 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:23 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:24 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:25 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:25 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:26 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 15:47:28 WARN DAGScheduler: Broadcasting large task binary with size 7.8 MiB\n",
      "23/11/28 15:47:28 WARN DAGScheduler: Broadcasting large task binary with size 7.8 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.4941011670521637\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "author_indexer = StringIndexer(inputCol=\"author_steamid\", outputCol=\"author_index\", handleInvalid=\"keep\")\n",
    "app_indexer = StringIndexer(inputCol=\"app_id\", outputCol=\"app_index\", handleInvalid=\"keep\")\n",
    "als = ALS(maxIter=2, regParam=0.01, userCol=\"author_index\", itemCol=\"app_index\", ratingCol=\"author_playtime_forever\", coldStartStrategy=\"drop\", implicitPrefs=True)\n",
    "pipeline = Pipeline(stages=[author_indexer, app_indexer, als])\n",
    "\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"author_playtime_forever\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/28 16:00:17 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "23/11/28 16:00:18 WARN DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+----------+\n",
      "|author_index|app_index|prediction|\n",
      "+------------+---------+----------+\n",
      "|         101|      1.0| 0.9418529|\n",
      "|         101|      0.0| 0.8790058|\n",
      "+------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's say the new author_steamid is 'new_user' and they have played app_id 1, 2, and 3.\n",
    "new_user_data = spark.createDataFrame(\n",
    "    [('new_user', '294100'), ('new_user', '812140')],\n",
    "    ['author_steamid', 'app_id']\n",
    ")\n",
    "\n",
    "# Since 'new_user' is not in the training set, we will assign a new ID manually.\n",
    "# For example, if the highest ID in your training set was 100, you could assign 101.\n",
    "# This is a workaround and predictions for this user may not be reliable.\n",
    "new_user_data = new_user_data.withColumn(\"author_index\", F.lit(101))\n",
    "\n",
    "app_indexer_model = model.stages[1]\n",
    "als_model = model.stages[2]\n",
    "\n",
    "# Now we need to transform the app_id using the StringIndexer model used in training.\n",
    "# This will add a new column 'app_index' to our DataFrame.\n",
    "new_user_data_transformed = app_indexer_model.transform(new_user_data)\n",
    "\n",
    "# Now we can use the trained ALS model to make predictions for the new user.\n",
    "# Note that the 'app_indexer_model' should be the StringIndexerModel obtained from the pipeline for 'app_id', \n",
    "# which has already been fit to the training data.\n",
    "new_user_predictions = als_model.transform(new_user_data_transformed)\n",
    "\n",
    "# Show the predictions\n",
    "new_user_predictions.select(\"author_index\", \"app_index\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/28 15:23:26 ERROR Instrumentation: java.lang.IllegalArgumentException: requirement failed: Column author_steamid must be of type numeric but was actually of type string.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.util.SchemaUtils$.checkNumericType(SchemaUtils.scala:78)\n",
      "\tat org.apache.spark.ml.recommendation.ALSParams.validateAndTransformSchema(ALS.scala:259)\n",
      "\tat org.apache.spark.ml.recommendation.ALSParams.validateAndTransformSchema$(ALS.scala:257)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.validateAndTransformSchema(ALS.scala:616)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.transformSchema(ALS.scala:753)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:715)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:714)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column author_steamid must be of type numeric but was actually of type string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexto/Documents/ProgrammingProjects/Steam-Analysis/Alex/CollaborativeFiltering.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexto/Documents/ProgrammingProjects/Steam-Analysis/Alex/CollaborativeFiltering.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Build the recommendation model using ALS on the training data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexto/Documents/ProgrammingProjects/Steam-Analysis/Alex/CollaborativeFiltering.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexto/Documents/ProgrammingProjects/Steam-Analysis/Alex/CollaborativeFiltering.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m als \u001b[39m=\u001b[39m ALS(maxIter\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, regParam\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexto/Documents/ProgrammingProjects/Steam-Analysis/Alex/CollaborativeFiltering.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m           userCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauthor_steamid\u001b[39m\u001b[39m\"\u001b[39m, itemCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapp_id\u001b[39m\u001b[39m\"\u001b[39m, ratingCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauthor_playtime_forever\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexto/Documents/ProgrammingProjects/Steam-Analysis/Alex/CollaborativeFiltering.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m           coldStartStrategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdrop\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexto/Documents/ProgrammingProjects/Steam-Analysis/Alex/CollaborativeFiltering.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m           implicitPrefs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexto/Documents/ProgrammingProjects/Steam-Analysis/Alex/CollaborativeFiltering.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m als\u001b[39m.\u001b[39;49mfit(training)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexto/Documents/ProgrammingProjects/Steam-Analysis/Alex/CollaborativeFiltering.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Evaluate the model by computing the RMSE on the test data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexto/Documents/ProgrammingProjects/Steam-Analysis/Alex/CollaborativeFiltering.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtransform(test)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column author_steamid must be of type numeric but was actually of type string."
     ]
    }
   ],
   "source": [
    "# # Build the recommendation model using ALS on the training data\n",
    "# # Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "# als = ALS(maxIter=2, regParam=0.01, \n",
    "#           userCol=\"author_steamid\", itemCol=\"app_id\", ratingCol=\"author_playtime_forever\",\n",
    "#           coldStartStrategy=\"drop\",\n",
    "#           implicitPrefs=True)\n",
    "# model = als.fit(training)\n",
    "\n",
    "# # Evaluate the model by computing the RMSE on the test data\n",
    "# predictions = model.transform(test)\n",
    "# evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"watching_percentage\",\n",
    "#                                 predictionCol=\"prediction\")\n",
    "\n",
    "# rmse = evaluator.evaluate(predictions)\n",
    "# print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
